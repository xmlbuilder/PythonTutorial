# ğŸŒ AsyncIO ë©€í‹° ì›¹ ìŠ¤í¬ë˜í•‘ + HTML íŒŒì‹± ì‹¤ìŠµ
## ğŸ¯ ëª©ì 
- ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë™ì‹œì— ìš”ì²­í•˜ê³  HTMLì„ íŒŒì‹±í•˜ì—¬ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì¶”ì¶œ
- asyncio + ThreadPoolExecutor + BeautifulSoup ì¡°í•©ìœ¼ë¡œ ë¹„ë™ê¸° ì›¹ ìŠ¤í¬ë˜í•‘ êµ¬í˜„


## ğŸ§  í•µì‹¬ ê°œë…

| ê°œë…               | ì„¤ëª…                                                                 |
|--------------------|----------------------------------------------------------------------|
| `asyncio`          | Pythonì˜ ë¹„ë™ê¸° ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬. ì½”ë£¨í‹´ ê¸°ë°˜ìœ¼ë¡œ non-blocking ì‹¤í–‰ ê°€ëŠ¥ |
| `run_in_executor()`| ë™ê¸° í•¨ìˆ˜ë¥¼ ë³„ë„ ì“°ë ˆë“œ/í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¹„ë™ê¸°ì²˜ëŸ¼ ì²˜ë¦¬í•¨         |
| `ThreadPoolExecutor`| ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë³‘ë ¬ ì‘ì—…ì„ ì²˜ë¦¬í•  ì“°ë ˆë“œ í’€ ìƒì„±                      |
| `BeautifulSoup`    | HTML íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬. ì›í•˜ëŠ” íƒœê·¸ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ ì¶”ì¶œ ê°€ëŠ¥             |
| `asyncio.gather()` | ì—¬ëŸ¬ ì½”ë£¨í‹´ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜                      |


## ğŸ§ª í†µí•© ì‹¤ìŠµ ì½”ë“œ
```python
import asyncio
import timeit
from urllib.request import urlopen
from concurrent.futures import ThreadPoolExecutor
import threading
from bs4 import BeautifulSoup

# ì‹¤í–‰ ì‹œì‘ ì‹œê°„
start = timeit.default_timer()

# ëŒ€ìƒ ì‚¬ì´íŠ¸ ëª©ë¡
urls = [
    'http://daum.net',
    'https://naver.com',
    'http://mlbpark.donga.com/',
    'https://tistory.com',
    'https://wemakeprice.com/'
]

# ê°œë³„ URL ì²˜ë¦¬ í•¨ìˆ˜
async def fetch(url, executor):
    print('Thread Name :', threading.current_thread().name, 'Start', url)
    
    # urlopenì€ ë¸”ë¡œí‚¹ í•¨ìˆ˜ â†’ executorë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
    res = await loop.run_in_executor(executor, urlopen, url)
    
    # HTML íŒŒì‹±
    soup = BeautifulSoup(res.read(), 'html.parser')
    
    print('Thread Name :', threading.current_thread().name, 'Done', url)
    
    # <title> íƒœê·¸ ì¶”ì¶œ
    return soup.title.string if soup.title else 'No Title Found'

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def main():
    executor = ThreadPoolExecutor(max_workers=10)
    
    # ëª¨ë“  ì‘ì—…ì„ futureë¡œ ìŠ¤ì¼€ì¤„ë§
    futures = [asyncio.ensure_future(fetch(url, executor)) for url in urls]
    
    # ê²°ê³¼ ì·¨í•©
    results = await asyncio.gather(*futures)
    
    print('\nğŸ“„ Parsed Titles:')
    for url, title in zip(urls, results):
        print(f'- {url} â†’ {title}')

# ì‹¤í–‰
if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
    
    duration = timeit.default_timer() - start
    print(f'\nâ±ï¸ Total Running Time: {duration:.2f} seconds')
```


## ğŸ§­ AsyncIO ì›¹ ìŠ¤í¬ë˜í•‘ + íŒŒì‹± ì ˆì°¨ íë¦„ ìš”ì•½
### â‘  ì¤€ë¹„ ë‹¨ê³„: ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì´ˆê¸° ì„¤ì •
- í•„ìš”í•œ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¨ë‹¤:
- asyncio: ë¹„ë™ê¸° ì½”ë£¨í‹´ ì‹¤í–‰
- timeit: ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- urlopen: ì›¹ ìš”ì²­ (ë™ê¸° ë°©ì‹)
- ThreadPoolExecutor: ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ì²˜ëŸ¼ ì‹¤í–‰
- BeautifulSoup: HTML íŒŒì‹±
- threading: í˜„ì¬ ì“°ë ˆë“œ í™•ì¸ìš© (ë””ë²„ê¹… ëª©ì )
```python
import asyncio, timeit, threading
from urllib.request import urlopen
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
```


### â‘¡ íƒ€ê²Ÿ URL ëª©ë¡ ì •ì˜
- ë™ì‹œì— ìš”ì²­í•  ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜
```python
urls = ['http://daum.net', 'https://naver.com', ...]

```

### â‘¢ ë¹„ë™ê¸° ì‘ì—… í•¨ìˆ˜ ì •ì˜ (fetch)
- ê° URLì— ëŒ€í•´ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰:
- í˜„ì¬ ì“°ë ˆë“œ ì´ë¦„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
- urlopen()ì„ run_in_executor()ë¡œ ë¹„ë™ê¸° ì‹¤í–‰
- ì‘ë‹µ HTMLì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
- <title> íƒœê·¸ ì¶”ì¶œ í›„ ë°˜í™˜
```python
async def fetch(url, executor):
    res = await loop.run_in_executor(executor, urlopen, url)
    soup = BeautifulSoup(res.read(), 'html.parser')
    return soup.title.string if soup.title else 'No Title Found'
```


### â‘£ ë©”ì¸ ì½”ë£¨í‹´ ì •ì˜ (main)
- ì „ì²´ ì‘ì—…ì„ ê´€ë¦¬í•˜ëŠ” ì½”ë£¨í‹´:
- ThreadPoolExecutor ìƒì„±
- ê° URLì— ëŒ€í•´ fetch() ì½”ë£¨í‹´ì„ ensure_future()ë¡œ ìŠ¤ì¼€ì¤„ë§
- asyncio.gather()ë¡œ ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ ì‹¤í–‰
- ê²°ê³¼ ì¶œë ¥
```python
async def main():
    executor = ThreadPoolExecutor(max_workers=10)
    futures = [asyncio.ensure_future(fetch(url, executor)) for url in urls]
    results = await asyncio.gather(*futures)
```


### â‘¤ ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
- loop.run_until_complete(main())ìœ¼ë¡œ ì½”ë£¨í‹´ ì‹¤í–‰
- timeitìœ¼ë¡œ ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
```python
loop = asyncio.get_event_loop()
loop.run_until_complete(main())
duration = timeit.default_timer() - start

```


### âœ… ì „ì²´ íë¦„ ìš”ì•½

| ë‹¨ê³„            | ì„¤ëª…                                                                 |
|-----------------|----------------------------------------------------------------------|
| ì¤€ë¹„            | í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ë° URL ë¦¬ìŠ¤íŠ¸ ì •ì˜                                 |
| `fetch()`       | ê° URLì— ëŒ€í•´ ì›¹ ìš”ì²­ ì‹¤í–‰ (`urlopen`) + HTML íŒŒì‹± (`BeautifulSoup`) |
| `main()`        | ëª¨ë“  `fetch()` ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ìŠ¤ì¼€ì¤„ë§í•˜ê³  `gather()`ë¡œ ì‹¤í–‰         |
| ê²°ê³¼ ì¶”ì¶œ       | ê° HTML ë¬¸ì„œì—ì„œ `<title>` íƒœê·¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜         |
| ì‹¤í–‰ ë° ì¸¡ì •    | `loop.run_until_complete()`ë¡œ ì½”ë£¨í‹´ ì‹¤í–‰ + ì „ì²´ ìˆ˜í–‰ ì‹œê°„ ì¸¡ì •        |




## ğŸ“Œ ì°¸ê³  ì‚¬í•­
- urlopen()ì€ ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ aiohttpë¡œ ì „í™˜í•˜ë©´ ë” íš¨ìœ¨ì 
- BeautifulSoupì€ HTML êµ¬ì¡°ê°€ ë³µì¡í•œ ì‚¬ì´íŠ¸ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ íŒŒì‹± ê°€ëŠ¥
- run_in_executor()ëŠ” ê¸°ì¡´ ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ì²˜ëŸ¼ ì‹¤í–‰í•  ë•Œ ìœ ìš©
- asyncio.gather()ëŠ” ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼


## ì „ì²´ ì½”ë“œ
```python
import asyncio
import timeit
from urllib.request import urlopen
from concurrent.futures import ThreadPoolExecutor
import threading

# ì‹¤í–‰ ì‹œì‘ ì‹œê°„
start = timeit.default_timer()

# ì„œë¹„ìŠ¤ ë°©í–¥ì´ ë¹„ìŠ·í•œ ì‚¬ì´íŠ¸ë¡œ ì‹¤ìŠµ ê¶Œì¥(ì˜ˆ : ê²Œì‹œíŒì„± ì»¤ë®¤ë‹ˆí‹°)
urls = ['http://daum.net', 'https://naver.com', 'http://mlbpark.donga.com/', 'https://tistory.com', 'https://wemakeprice.com/']

async def fetch(url, executor):
    # ì“°ë ˆë“œëª… ì¶œë ¥
    print('Thread Name :', threading.current_thread().name, 'Start', url)

    # ì‹¤í–‰
    res = await loop.run_in_executor(executor, urlopen, url)
    print('Thread Name :', threading.current_thread().name, 'Done', url)

    # ê²°ê³¼ ë°˜í™˜
    return res.read()[0:5]

async def main():

    # ì“°ë ˆë“œ í’€ ìƒì„±
    executor = ThreadPoolExecutor(max_workers=10)

    # future ê°ì²´ ëª¨ì•„ì„œ gatherì—ì„œ ì‹¤í–‰
    futures = [
        asyncio.ensure_future(fetch(url, executor)) for url in urls
    ]
    
    # ê²°ê³¼ ì·¨í•©
    rst = await asyncio.gather(*futures)
    print()

    print('Result : ', rst)

if __name__ == '__main__':
    # ë£¨í”„ ì´ˆê¸°í™”
    loop = asyncio.get_event_loop()

    # ì‘ì—… ì™„ë£Œ ê¹Œì§€ ëŒ€ê¸°
    loop.run_until_complete(main())

    # ìˆ˜í–‰ ì‹œê°„ ê³„ì‚°
    duration = timeit.default_timer() - start
    # ì´ ì‹¤í–‰ ì‹œê°„
    print('Total Running Time : ', duration)

```
# loop.run_in_executor /asyncio.ensure_future/asyncio.gather/asyncio.get_event_loop

asyncioë¥¼ í™œìš©í•œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ ìì£¼ ì“°ì´ëŠ” í•µì‹¬ í•¨ìˆ˜ë“¤â€”loop.run_in_executor, asyncio.ensure_future,  
asyncio.gather, asyncio.get_event_loopâ€”ì´ ê°ê° ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì ˆì°¨ì ìœ¼ë¡œ ì •ë¦¬í•´ë³¼ê²Œ.

## ğŸ§  AsyncIO í•µì‹¬ í•¨ìˆ˜ ì„¤ëª…
### â‘  asyncio.get_event_loop()
- ì—­í• : í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©í•  ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê°€ì ¸ì˜´
- ì„¤ëª…: ëª¨ë“  ì½”ë£¨í‹´ì€ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ, ë£¨í”„ë¥¼ ë¨¼ì € ê°€ì ¸ì™€ì•¼ í•¨
- ì‚¬ìš© ì˜ˆì‹œ:

```python
loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```


### â‘¡ loop.run_in_executor(executor, func, *args)
- ì—­í• : ë™ê¸° í•¨ìˆ˜ë¥¼ ë³„ë„ì˜ ì“°ë ˆë“œë‚˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¹„ë™ê¸°ì²˜ëŸ¼ ì²˜ë¦¬
- ì„¤ëª…:
- urlopen() ê°™ì€ ë¸”ë¡œí‚¹ í•¨ìˆ˜ëŠ” ì§ì ‘ awaití•  ìˆ˜ ì—†ìŒ
- ëŒ€ì‹  run_in_executor()ë¡œ ì“°ë ˆë“œ í’€ì— ë„˜ê²¨ì„œ ë³‘ë ¬ ì²˜ë¦¬
- ì‚¬ìš© ì˜ˆì‹œ:
```python
res = await loop.run_in_executor(executor, urlopen, url)
```


### â‘¢ asyncio.ensure_future(coroutine)
- ì—­í• : ì½”ë£¨í‹´ì„ Future ê°ì²´ë¡œ ê°ì‹¸ì„œ ìŠ¤ì¼€ì¤„ë§
- ì„¤ëª…:
- ì½”ë£¨í‹´ì„ ì‹¤í–‰ ëŒ€ê¸°ì—´ì— ë“±ë¡
- gather()ë‚˜ wait() ê°™ì€ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„
- ì‚¬ìš© ì˜ˆì‹œ:
```python
futures = [asyncio.ensure_future(fetch(url, executor)) for url in urls]
```

###  â‘£ asyncio.gather(*futures)
- ì—­í• : ì—¬ëŸ¬ ì½”ë£¨í‹´ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
- ì„¤ëª…:
- ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
- ê²°ê³¼ëŠ” ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ë°˜í™˜ë¨
- ì‚¬ìš© ì˜ˆì‹œ:
```python
results = await asyncio.gather(*futures)
```


### âœ… ì „ì²´ íë¦„ ìš”ì•½
| ë‹¨ê³„               | í•¨ìˆ˜                   | ì—­í•  ì„¤ëª…                                                                 |
|--------------------|------------------------|---------------------------------------------------------------------------|
| ì´ë²¤íŠ¸ ë£¨í”„ ì¤€ë¹„   | `get_event_loop()`     | í˜„ì¬ ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©í•  ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê°€ì ¸ì˜´                               |
| ë™ê¸° í•¨ìˆ˜ ë¹„ë™ê¸°í™” | `run_in_executor()`    | ë¸”ë¡œí‚¹ í•¨ìˆ˜ë¥¼ ì“°ë ˆë“œ/í”„ë¡œì„¸ìŠ¤ í’€ì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¹„ë™ê¸°ì²˜ëŸ¼ ì²˜ë¦¬í•¨           |
| ì‘ì—… ìŠ¤ì¼€ì¤„ë§      | `ensure_future()`      | ì½”ë£¨í‹´ì„ `Future` ê°ì²´ë¡œ ê°ì‹¸ì„œ ì´ë²¤íŠ¸ ë£¨í”„ì— ë“±ë¡                        |
| ë³‘ë ¬ ì‹¤í–‰ ë° ìˆ˜ì§‘  | `gather()`             | ì—¬ëŸ¬ `Future`ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜                       |

---

## ğŸ Blackboxí™” ì „ëµ: ì‚¬ìš©ì ì¹œí™”ì  êµ¬ì¡° ë§Œë“¤ê¸°
### âœ… 1. í•µì‹¬ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ê°ì‹¸ê¸°

ë³µì¡í•œ íë¦„ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ì¶”ìƒí™”í•˜ë©´, ì‚¬ìš©ìëŠ” ë‚´ë¶€ êµ¬ì¡°ë¥¼ ëª°ë¼ë„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.
def scrape_titles(urls):
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(_run_scraper(urls))


### âœ… 2. ë‚´ë¶€ ì½”ë£¨í‹´ì€ ìˆ¨ê¸°ê¸°
ì‹¤ì œ fetch, main, executor ë“±ì€ ë‚´ë¶€ì—ì„œë§Œ ë™ì‘í•˜ê²Œ í•˜ê³ , ì™¸ë¶€ì—ëŠ” ë‹¨ìˆœí•œ ì¸í„°í˜ì´ìŠ¤ë§Œ ì œê³µ.
```python
async def _run_scraper(urls):
    executor = ThreadPoolExecutor(max_workers=10)
    futures = [asyncio.ensure_future(_fetch(url, executor)) for url in urls]
    return await asyncio.gather(*futures)

async def _fetch(url, executor):
    res = await loop.run_in_executor(executor, urlopen, url)
    soup = BeautifulSoup(res.read(), 'html.parser')
    return soup.title.string if soup.title else 'No Title Found'
```            

### âœ… 3. ì‚¬ìš©ìëŠ” ì´ë ‡ê²Œë§Œ ì“°ë©´ ë¨
```
urls = ['https://naver.com', 'https://daum.net']
titles = scrape_titles(urls)
print(titles)
```


### ğŸ§  ì¥ì 

| í•­ëª©             | ì„¤ëª…                                                                 |
|------------------|----------------------------------------------------------------------|
| ì‚¬ìš©ì„± í–¥ìƒ      | ë³µì¡í•œ ë¹„ë™ê¸° íë¦„ì„ í•¨ìˆ˜ í•˜ë‚˜ë¡œ ê°ì‹¸ ì‚¬ìš©ìì—ê²Œ ë‹¨ìˆœí•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ |
| ìœ ì§€ë³´ìˆ˜ ìš©ì´    | ë‚´ë¶€ ë¡œì§ ë³€ê²½ ì‹œ ì™¸ë¶€ ì‚¬ìš© ë°©ì‹ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ ê°€ëŠ¥                   |
| í…ŒìŠ¤íŠ¸ í¸ë¦¬      | ë‹¨ì¼ í•¨ìˆ˜ ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•˜ì—¬ ì•ˆì •ì„± í™•ë³´                          |
| í™•ì¥ì„± í™•ë³´      | ì˜ˆì™¸ ì²˜ë¦¬, ë¡œê¹…, íŒŒì‹± ë°©ì‹ ë³€ê²½ ë“± ìœ ì—°í•˜ê²Œ ì¶”ê°€ ê°€ëŠ¥                 |
| `aiohttp` ì „í™˜   | `urlopen` ëŒ€ì‹  `aiohttp` ì‚¬ìš© ì‹œ ì§„ì •í•œ ë¹„ë™ê¸° HTTP ìš”ì²­ ê°€ëŠ¥          |

---


